\begin{abstract}
Benchmarks are essential for quantitatively tracking progress in AI. As AI 
agents become increasingly capable, researchers and practitioners have 
introduced \textit{agentic benchmarks} to evaluate agents on complex, 
real-world tasks. \revise{These benchmarks typically measure agent capabilities by 
evaluating task outcomes via specific reward designs. However, we show that 
many agentic benchmarks have issues in task setup or reward design.} For 
example, \swebench-Verified uses insufficient test cases, while \taubench
counts empty responses as successful. Such issues can lead to under- or 
overestimation of agents' performance by up to 100\% in relative terms. To make 
agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist 
(\name), a set of guidelines that we synthesized from our benchmark-building 
experience, a survey of best practices, and previously reported issues. When 
applied to \cvebench, a benchmark with a particularly complex evaluation design, 
\name reduces the performance overestimation by 33\%.

\end{abstract}
