\section{Case Study} \label{sec:cases}

We present case study of specific issues we identified. For each study, we use
an Intel E5-2630 CPU with 128 GB RAM and optionally 1 NVIDIA H100 80GB for 
GPU-required experiments. We release our code at \url{https://github.com/uiuc-kang-lab/agentic-benchmarks}.

\subsection{\swebench}

\minihead{Benchmark Overview}
\swebench is a benchmark for evaluating the ability of AI agents to resolve 
real-world GitHub issues. Given the issue description and a summary of the 
codebase, agents are tasked with generating a patch that resolves the issue. 
Each generated patch is evaluated via existing unit tests in the GitHub 
repository.

\minihead{Identified Issue}
SWE-bench uses manually written unit tests to evaluate the correctness of a 
generated code patch. As illustrated in prior work, UTBoost \cite{utboost}, unit 
tests can lead to many false positives, due to the insufficiency of test cases.

\minihead{Example}
The Python package \texttt{seaborn} has an issue in handling missing values in 
the inputs \texttt{x} and \texttt{y} when computing polynomial fits using 
\texttt{PolyFit()}. Unfortunately, the unit test case for \texttt{PolyFit()} 
only considers the scenarios when both \texttt{x} and \texttt{y} have missing 
values:

\begin{lstlisting}[language=Python]
def test_missing_data(self, df):
    groupby = GroupBy([ "group" ])
    df.iloc[5:10] = np.nan
    res1 = PolyFit()( df[[ "x", "y" ]], groupby, "x", {})
    res2 = PolyFit()( df[[ "x" , "y" ]].dropna (), groupby, "x", {})
    assert_frame_equal( res1, res2 )
\end{lstlisting}

This insufficient test case for \texttt{PolyFit()} leads to the following 
incorrect patch for \texttt{PolyFit()} being evaluated as correct. This patch is 
generated by IBM SWE-1.0.

\begin{lstlisting}[language=Python]
def _fit_predict(self, data) :
    y = data ["y"].dropna()
    x = data ["x"].dropna()
    if x.shape[0] != y.shape[0]:
        raise ValueError("x and y must have the same number of non-missing values")
    if x.nunique() <= self.order :
        # TODO warn ?
    xx = yy = []
\end{lstlisting}

\minihead{Qualitative Results}
As reported in prior work \cite{utboost}, agents can pass evaluations without 
addressing the GitHub issues for 5.3\% and 7.7\% of tasks in the Verified and 
Lite partitions, respectively. These tasks lead to 40.9\% and 24.4\% changes in 
the leaderboard for the Verified and Lite partitions, respectively. Furthermore,
these tasks causes 2.3\% and 1.6\% overestimation of agent performance for
the Verified and Lite partitions, respectively.

\subsection{\taubench}

\minihead{Benchmark Overview} 
\taubench is for evaluation AI agents capability to interact with human users 
and follow domain-specific rules \cite{yaotau}. Given a domain-specific policy, the AI agent 
is tasked to interact with human users and answer user queries.

\minihead{Identified Issue} 
\taubench evaluates the agents' actions based on whether the database state is 
correct and optionally whether the agents' responses contain required text. 
Therefore, on tasks that do not change the database state and do not have 
required texts, agents can get positive evaluation results by doing nothing. On 
tasks that do not change the database state and has a trivial required text, 
such as "4", agents can get positive evaluation results by returning random 
responses or all the data.

\minihead{Example}
A task in \taubench requires agent to process a flight cancellation and refund 
request. An AI agent is supposed to check the detail of the booked flight 
ticket for the user in the database and deny the user request if the ticket is 
non-refundable. This task has no required output. Therefore, as long as the 
data state does not change, the agent will obtain a positive evaluation result. 
In this case, an agent that does nothing can also have a positive evaluation
result.

\minihead{Qualitative Results}
A do-nothing agent that returns immediately can achieve a 38\% and 6.0\% 
pass\verb|^|k or pass@k for any k for Airline and Retail partitions, 
respectively. A spamming agent that outputs all the data can achieve a 40\% and
9.6\% pass\verb|^|k or pass@k for any k for Airline and Retail partitions, 
respectively.

\subsection{\birdbench}

\minihead{Benchmark Overview}
\birdbench is for evaluating the capability of agents to write SQL queries 
\cite{li2023can}. Given a query description in natural language, the agent 
needs to translate it into a SQL query.

\minihead{Identified Issue} 
\birdbench evaluates agent by comparing the execution results of the ground truth
query with the generated query. However, due to the ambiguity of the query 
description, there can be multiple correct queries for the same natural language
description.

\minihead{Example}
A task in \birdbench asks the agent to write a SQL query that can answer the 
question: ``What are the name, independence year, and surface area of the 
country with the smallest population?'' There can be two correct SQL queries:
\begin{lstlisting}[language=SQL]
-- Query 1
SELECT Name, SurfaceArea, IndepYear FROM country 
WHERE Population = (SELECT min(Population) FROM country)
-- Query 2
SELECT Name, SurfaceArea, IndepYear FROM country
ORDER BY Population LIMIT 1
\end{lstlisting}
Query 1 outputs all the country with the smallest population, while Query 2 
outputs one of the country with the smallest population. Although the output of 
two queries are different, they both answer the question.

\subsection{\swelancer}

\minihead{Benchmark Overview}
\swelancer is for evaluating the capability of AI agents to independently 
implement features and fix bugs. \cite{miserendino2025swe} Given a task 
description, agent needs to use Python scripts to interact with the file system 
and modify codebase. 

\minihead{Identified Issue}
\swelancer uses end-to-end testing to evaluate the correctness of agents' 
implementation. Although the test cases are stored in a password-protected 
\texttt{.zip} file, reading the directory structure and updating files within 
the \texttt{.zip} file do not require a password. Therefore, an agent can easily 
locate the test cases and replace them with a naive one, such as 
``\texttt{assert 1==1}''. 

\minihead{Qualitative Results}
An agent that overwrites the test cases in the \texttt{.zip} file can achieve a 
100\% resolve rate without completing the software engineering tasks.

\subsection{\webarena}

\minihead{Benchmark Overview}
\webarena is for evaluating the capability of agents to interact with the web 
\cite{zhouwebarena}. Given a user request, the AI agent need either retrieve the 
required information or fill the given data into the web form correctly.

\minihead{Identified Issue} 
\webarena uses exact string matching, substring matching, and LLM-as-a-Judge to 
evaluate agents. Its strategy of exact string matching cannot handle 
alternative expressions and phrase modifiers, while the substring matching is 
vulnerable to exhaustive enumeration of the content on the website. 
Additionally, LLM-as-a-Judge can produce unreliable results.

\minihead{Example}
In \webarena, there is a user query that asks ``What is the duration required to 
first walk from Massachusetts Institute of Technology to Harvard University, and 
then drive to Boston Logan International Airport?'' The ground truth answer for 
this question is 63 minutes. However, the agent searched the web and output the 
final answer: ``The duration required to first walk from Massachusetts Institute 
of Technology to Harvard University is 45 minutes, and then drive to Boston 
Logan International Airport is 8 minutes.'' The answer of agent gives the 
duration of 45+8=53 minutes, which is different from the ground truth answer. 
However, the LLM judge considers the agent's answer as correct.

\subsection{\kernelbench}

\minihead{Benchmark Overview}
\kernelbench is for evaluating the capability of agents to write correct and 
efficient GPU kernels \cite{ouyang2025kernelbench}. Given the task instruction
and the original PyTorch code, agents need to write PyTorch code containing
an inline implementation of the kernel that is functionally correct and more 
efficient.

\minihead{Identified Issue 1}
\kernelbench uses randomly generated inputs (\ie, fuzzing) to test the 
correctness of generated GPU kernels. However, we find the tested functions in a 
subset of tasks are not sensitive to uniform random inputs, such as 
\texttt{mean(softmax(x))} and \texttt{relu(x-2)}. 

\minihead{Identified Issue 2}
In the evaluation implementation, \kernelbench first runs the ground truth 
kernel and then runs the generated kernel subsequently. As reported in prior 
work \cite{lange2025ai}, agents can potentially cheat by generating a program 
that extracts the execution results of the ground truth kernel.

\minihead{Identified Issue 3}
The fuzzer designed in \kernelbench fails to address potential inputs with 
different memory layouts (\eg, non-contiguous tensors), tensor shapes, and 
hardware environment. In the following code snippet, we demonstrate an incorrect
kernel function due to improper use of threads, which were graded as correct in
\kernelbench. In Line 46, the kernel function accesses parallel execution results
in \texttt{s\_sum} with index from \texttt{tid} to \texttt{nthread}. However, 
when \texttt{nthread > normalized\_size}, this will lead to out-of-bound access 
into uninitialized memory. Namely, a thread-safe guard is required here. 

{\scriptsize
\begin{lstlisting}[language=C++]
#include ...

template <typename scalar_t>
__global__ void layernorm_forward_kernel_opt(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    const float eps,
    scalar_t* __restrict__ output,
    const int normalized_size) {

  // Each block processes one outer instance.
  int instance_idx = blockIdx.x;

  // Use 2D thread indexing to cover the normalized dimension flexibly.
  int tid = threadIdx.y * blockDim.x + threadIdx.x;
  int nthreads = blockDim.x * blockDim.y;

  // Pointers to the start of this instance's data.
  const scalar_t* __restrict__ in_ptr = input + instance_idx * normalized_size;
  scalar_t* __restrict__ out_ptr = output + instance_idx * normalized_size;

  using accscalar_t = at::acc_type<scalar_t, true>;

  // Each thread computes a partial sum and sum of squares over a strided range.
  accscalar_t local_sum = 0;
  accscalar_t local_sum_sq = 0;
  for (int i = tid; i < normalized_size; i += nthreads) {
    // Use __ldg for read-only, coalesced global memory access
    scalar_t val = __ldg(&in_ptr[i]);
    accscalar_t a_val = static_cast<accscalar_t>(val);
    local_sum += a_val;
    local_sum_sq += a_val * a_val;
  }

  // Allocate shared memory for reduction: first part for partial sums, second for sum of squares.
  extern __shared__ char smem[];
  accscalar_t* s_sum = reinterpret_cast<accscalar_t*>(smem);
  accscalar_t* s_sum_sq = s_sum + nthreads;

  s_sum[tid] = local_sum;
  s_sum_sq[tid] = local_sum_sq;
  __syncthreads();

  // Perform parallel reduction in shared memory.
  for (int stride = nthreads / 2; stride > 0; stride >>= 1) {
    if (tid < stride) {
      s_sum[tid] += s_sum[tid + stride];
      s_sum_sq[tid] += s_sum_sq[tid + stride];
    }
    __syncthreads();
  }
...
}
\end{lstlisting}
}

To identify such issues in large scale, we applied o3-mini to generate additional
test cases. Specifically, we sampled 3 generated kernel functions for each task 
in level 1 and asked o3-mini to detect any possible flaws and write test cases
for each detected flaw. Then, we manually verified the correctness of 
o3-mini-generated test cases. Finally, we applied these test cases on all 
generations by \citet{lange2025ai}. Our results show that the correctness rate
of generated kernels is overestimated by 31\%.