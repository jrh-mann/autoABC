\section{Limitation and Impact Statement} \label{sec:app-limit}

\minihead{Limitation} 
As the first study to systematically investigate the issue of evaluation rigor 
in agentic benchmarks, our work is not without limitations. First, our analysis
covered only 17 agentic benchmarks that are used by top AI providers between 
January 2024 and March 2025. We did not analyze benchmarks outside this time 
frame. Therefore, our findings may not necessarily include all relevant 
evaluation practices. Consequently, it is possible that we have not presented an 
exhaustive checklist for ensuring evaluation rigor.
Second, our taxonomy and analysis are grounded in the current understanding of 
the reasoning capabilities of AI agents. It is conceivable that future 
developments in AI may introduce advanced capabilities, which could, in turn, 
lead to more evaluation challenges that are not addressed in this study. Finally, 
our findings only reflect the state the analyzed benchmark at the time of 
writing. Future revisions of these benchmarks may yield different results. 
Therefore, our conclusions may not fully apply to subsequent versions.

\minihead{Broader Impact}
Although our study rigorously highlights shortcomings in existing benchmarks, 
our aim is not to criticize but to raise awareness and foster the development of 
a stronger community with higher standards and improved quality in agentic 
benchmarks. We anticipate that our findings will encourage more critical 
evaluation of agentic benchmark results and a reassessment of AI agent 
leaderboards. We believe these contributions will lead to a deeper and more 
accurate understanding of AI agent capabilities, resulting in positive societal 
impact.


