\section{Conclusion}
We formulate the first actionable agentic benchmarks checklists (\name) focusing 
on the outcome validity, task validity, and reporting of results. Via \name, we 
proposed a set of the best practices for building rigorous agentic benchmarks. 
Based on \name, we assessed ten widely used agentic benchmarks and identified 
significant evaluation issues that cases up to 100\% errors (in relative terms) 
when estimating agents' performance. Finally, we use \cvebench \cite{zhu2025cve} 
as an example to demonstrate using \name to improve the evaluation rigor during 
benchmark construction.